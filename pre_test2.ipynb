{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 초기 환경 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0+cu101\n",
      "1.1.2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DATA Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0   1   2\n",
      "0        3_5_1123.jpg   3   5\n",
      "1       3_20_1048.jpg   3  20\n",
      "2         4_2_401.jpg   4   2\n",
      "3         4_7_740.jpg   4   7\n",
      "4         4_11_93.jpg   4  11\n",
      "...               ...  ..  ..\n",
      "15995  13_15_1600.jpg  13  15\n",
      "15996  13_16_1570.jpg  13  16\n",
      "15997   13_17_986.jpg  13  17\n",
      "15998  13_18_4980.jpg  13  18\n",
      "15999   13_20_282.jpg  13  20\n",
      "\n",
      "[16000 rows x 3 columns]\n",
      "             0\n",
      "0        0.jpg\n",
      "1        1.jpg\n",
      "2        2.jpg\n",
      "3        3.jpg\n",
      "4        4.jpg\n",
      "...        ...\n",
      "3992  3992.jpg\n",
      "3993  3993.jpg\n",
      "3994  3994.jpg\n",
      "3995  3995.jpg\n",
      "3996  3996.jpg\n",
      "\n",
      "[3997 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"./train.tsv\", sep='\\t', header=None)\n",
    "print(data_train)\n",
    "\n",
    "data_test = pd.read_csv(\"./test.tsv\", sep='\\t', header=None)\n",
    "print(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_label = {\n",
    "    (3, 5): 0,\n",
    "    (3, 20): 1,\n",
    "    (4, 2): 2,\n",
    "    (4, 7): 3,\n",
    "    (4, 11): 4,\n",
    "    (5, 8): 5,\n",
    "    (7, 1): 6,\n",
    "    (7, 20): 7,\n",
    "    (8, 6): 8,\n",
    "    (8, 9): 9,\n",
    "    (10, 20): 10,\n",
    "    (11, 14): 11,\n",
    "    (13, 1): 12,\n",
    "    (13, 6): 13,\n",
    "    (13, 9): 14,\n",
    "    (13, 15): 15,\n",
    "    (13, 16): 16,\n",
    "    (13, 17): 17,\n",
    "    (13, 18): 18,\n",
    "    (13, 20): 19\n",
    "}\n",
    "\n",
    "index2label = {v: k for k, v in combined_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tsv_path, data_path):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.data = self._preprocess(tsv_path)\n",
    "            \n",
    "    def _preprocess(self, tsv_path):\n",
    "        data = pd.read_csv(tsv_path, sep='\\t', header=None)\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Override 필수 함수\n",
    "    @lru_cache(maxsize=100000)\n",
    "    def __getitem__(self, index):\n",
    "        file_name = Path(self.data.iloc[index][0])\n",
    "        \n",
    "        if str(self.data_path) == 'train':\n",
    "            plant_label = self.data.iloc[index][1]\n",
    "            disease_label = self.data.iloc[index][2]\n",
    "        else:\n",
    "            plant_label = -1\n",
    "            disease_label = -1\n",
    "        \n",
    "        file_path = self.data_path / file_name\n",
    "        \n",
    "        data = Image.open(file_path)\n",
    "        data = asarray(data) # (H, W, C) \n",
    "        data = torch.FloatTensor(data)\n",
    "        data = data.permute(2, 0, 1).contiguous()  # (H, W, C) --> (C, H, W)\n",
    "        \n",
    "        # print(data, plant_label, disease_label)\n",
    "        \n",
    "        label = combined_label.get((plant_label, disease_label), -1)\n",
    "        \n",
    "        if str(self.data_path) == 'train' and label == -1:\n",
    "            raise Exception('Label Error!!')\n",
    "        \n",
    "        return data, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800 3200\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PlantImageDataset(tsv_path=\"./train.tsv\", data_path=\"train\")\n",
    "\n",
    "\n",
    "train_size = int(len(train_dataset) * 0.8)\n",
    "valid_size = int(len(train_dataset) * 0.2)\n",
    "assert len(train_dataset) == (train_size + valid_size)\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
    "print(len(train_dataset), len(valid_dataset))\n",
    "\n",
    "test_dataset = PlantImageDataset(tsv_path=\"./test.tsv\", data_path=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in test_loader:\n",
    "#     print(x, y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlantModel(\n",
      "  (pre_model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (8): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (9): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (10): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (11): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (12): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (13): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (14): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (15): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (16): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (17): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (18): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (19): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (20): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (21): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (22): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PlantModel(nn.Module):\n",
    "    def __init__(self, label_size=20):\n",
    "        super(PlantModel, self).__init__()\n",
    "\n",
    "        self.pre_model = models.resnext101_32x8d()\n",
    "        self.pre_model.fc = nn.Linear(2048, label_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pre_model(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = PlantModel()\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.wide_resnet50_2()\n",
    "# model.fc = nn.Linear(2048, 14)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.vgg16()\n",
    "# model.classifier._modules['6'] = nn.Linear(4096, 14)\n",
    "# # model.fc = nn.Linear(1000, 14)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer=None, device=None, epoch=None):\n",
    "    model.train() # 학습 모드\n",
    "    \n",
    "    total_loss = 0.\n",
    "    total_num = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "   \n",
    "    for batch, (data, label) in enumerate(train_loader):\n",
    "        # data\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hyp = model(data)\n",
    "        \n",
    "        loss = criterion(hyp, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "        total_num += 1\n",
    "        \n",
    "        log_interval = 100\n",
    "        if batch % log_interval == 0 and batch >= 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | grad_norm {:5.2f} | loss {:5.2f} | time {:.4f}'.format(\n",
    "                    epoch+1, batch+1, len(train_loader), grad_norm, loss.detach().item(), elapsed))\n",
    "            start_time = time.time()\n",
    "    \n",
    "    train_loss = total_loss / total_num\n",
    "    return train_loss\n",
    "        \n",
    "        \n",
    "def validate_one_epoch(model, valid_loader, criterion, optimizer=None, device=None, epoch=None, print_output=False):\n",
    "    model.eval() # 평가 모드\n",
    "    \n",
    "    total_loss = 0.\n",
    "    total_num = 0\n",
    "    \n",
    "    correct= 0\n",
    "    total_size = 0\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for batch, (data, label) in enumerate(valid_loader):\n",
    "            # data\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            hyp = model(data)\n",
    "        \n",
    "            loss = criterion(hyp, label)\n",
    "            \n",
    "            if print_output:\n",
    "                hyp = torch.argmax(hyp, dim=-1)\n",
    "                correct += (hyp == label).sum().item()\n",
    "\n",
    "            total_loss += loss.detach().item()\n",
    "            total_num += 1\n",
    "            total_size += data.size(0)\n",
    "    \n",
    "    valid_loss = total_loss / total_num\n",
    "    accuracy = correct / total_size\n",
    "    \n",
    "    return valid_loss, accuracy\n",
    "\n",
    "def save_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/TF2_37/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/  800 batches | grad_norm 11285.97 | loss 50.25 | time 0.5461\n",
      "| epoch   1 |   101/  800 batches | grad_norm 16.48 | loss 36.90 | time 47.8221\n",
      "| epoch   1 |   201/  800 batches | grad_norm 27.52 | loss 31.85 | time 48.5100\n",
      "| epoch   1 |   301/  800 batches | grad_norm 15.38 | loss 28.99 | time 46.8379\n",
      "| epoch   1 |   401/  800 batches | grad_norm 21.30 | loss 40.28 | time 46.5221\n",
      "| epoch   1 |   501/  800 batches | grad_norm 16.35 | loss 26.43 | time 46.7530\n",
      "| epoch   1 |   601/  800 batches | grad_norm 20.25 | loss 42.77 | time 46.4373\n",
      "| epoch   1 |   701/  800 batches | grad_norm 19.02 | loss 27.01 | time 46.8288\n",
      "| epoch   1 | train_loss : 38.3598\n",
      "| epoch   1 | valid_loss : 42.7795 accuracy : 0.2264\n",
      "| epoch   2 |     1/  800 batches | grad_norm 25.06 | loss 33.62 | time 0.4271\n",
      "| epoch   2 |   101/  800 batches | grad_norm 23.05 | loss 30.28 | time 41.8917\n",
      "| epoch   2 |   201/  800 batches | grad_norm 21.45 | loss 23.02 | time 41.2943\n",
      "| epoch   2 |   301/  800 batches | grad_norm 18.90 | loss 24.98 | time 41.1498\n",
      "| epoch   2 |   401/  800 batches | grad_norm 28.52 | loss 46.44 | time 41.0760\n",
      "| epoch   2 |   501/  800 batches | grad_norm 19.25 | loss 24.06 | time 41.2734\n",
      "| epoch   2 |   601/  800 batches | grad_norm 21.51 | loss 26.48 | time 40.9357\n",
      "| epoch   2 |   701/  800 batches | grad_norm 20.18 | loss 16.22 | time 41.9291\n",
      "| epoch   2 | train_loss : 26.5549\n",
      "| epoch   2 | valid_loss : 23.6742 accuracy : 0.5086\n",
      "| epoch   3 |     1/  800 batches | grad_norm 17.32 | loss 19.37 | time 0.4412\n",
      "| epoch   3 |   101/  800 batches | grad_norm 20.10 | loss 28.15 | time 41.8041\n",
      "| epoch   3 |   201/  800 batches | grad_norm 18.83 | loss 20.05 | time 41.3943\n",
      "| epoch   3 |   301/  800 batches | grad_norm 25.48 | loss 26.39 | time 41.5992\n",
      "| epoch   3 |   401/  800 batches | grad_norm 30.13 | loss 36.38 | time 41.5907\n",
      "| epoch   3 |   501/  800 batches | grad_norm 18.68 | loss 16.51 | time 41.7410\n",
      "| epoch   3 |   601/  800 batches | grad_norm 13.52 | loss  7.94 | time 41.4987\n",
      "| epoch   3 |   701/  800 batches | grad_norm 13.87 | loss 10.83 | time 41.2622\n",
      "| epoch   3 | train_loss : 17.2511\n",
      "| epoch   3 | valid_loss : 23.2261 accuracy : 0.5911\n",
      "| epoch   4 |     1/  800 batches | grad_norm 17.74 | loss  9.43 | time 0.4375\n",
      "| epoch   4 |   101/  800 batches | grad_norm 29.72 | loss 16.16 | time 42.7308\n",
      "| epoch   4 |   201/  800 batches | grad_norm 10.15 | loss  6.39 | time 42.4680\n",
      "| epoch   4 |   301/  800 batches | grad_norm 18.22 | loss 14.42 | time 41.1795\n",
      "| epoch   4 |   401/  800 batches | grad_norm 12.57 | loss  6.24 | time 41.1452\n",
      "| epoch   4 |   501/  800 batches | grad_norm 15.65 | loss  9.25 | time 41.1389\n",
      "| epoch   4 |   601/  800 batches | grad_norm 18.58 | loss  9.84 | time 40.9831\n",
      "| epoch   4 |   701/  800 batches | grad_norm 17.50 | loss 11.66 | time 41.2100\n",
      "| epoch   4 | train_loss : 10.8079\n",
      "| epoch   4 | valid_loss : 12.1628 accuracy : 0.7561\n",
      "| epoch   5 |     1/  800 batches | grad_norm 17.40 | loss  6.95 | time 0.4270\n",
      "| epoch   5 |   101/  800 batches | grad_norm 11.26 | loss  3.88 | time 41.8597\n",
      "| epoch   5 |   201/  800 batches | grad_norm 17.62 | loss 11.56 | time 41.0236\n",
      "| epoch   5 |   301/  800 batches | grad_norm 11.98 | loss  4.82 | time 40.8429\n",
      "| epoch   5 |   401/  800 batches | grad_norm 23.99 | loss 16.74 | time 41.6557\n",
      "| epoch   5 |   501/  800 batches | grad_norm 12.32 | loss  4.89 | time 42.3445\n",
      "| epoch   5 |   601/  800 batches | grad_norm 18.48 | loss 10.65 | time 42.4922\n",
      "| epoch   5 |   701/  800 batches | grad_norm  9.25 | loss  6.63 | time 41.6671\n",
      "| epoch   5 | train_loss :  8.1205\n",
      "| epoch   5 | valid_loss :  8.4567 accuracy : 0.8236\n",
      "| epoch   6 |     1/  800 batches | grad_norm  7.77 | loss  4.74 | time 0.4341\n",
      "| epoch   6 |   101/  800 batches | grad_norm 13.11 | loss  6.94 | time 41.5949\n",
      "| epoch   6 |   201/  800 batches | grad_norm  8.80 | loss  1.95 | time 41.1290\n",
      "| epoch   6 |   301/  800 batches | grad_norm 14.21 | loss  5.21 | time 41.2698\n",
      "| epoch   6 |   401/  800 batches | grad_norm  8.04 | loss  2.47 | time 41.1668\n",
      "| epoch   6 |   501/  800 batches | grad_norm  5.01 | loss  1.35 | time 41.2952\n",
      "| epoch   6 |   601/  800 batches | grad_norm 17.10 | loss 12.36 | time 41.0015\n",
      "| epoch   6 |   701/  800 batches | grad_norm 11.89 | loss  6.68 | time 41.0192\n",
      "| epoch   6 | train_loss :  6.6958\n",
      "| epoch   6 | valid_loss :  4.4463 accuracy : 0.9083\n",
      "| epoch   7 |     1/  800 batches | grad_norm 13.26 | loss  6.33 | time 0.4256\n",
      "| epoch   7 |   101/  800 batches | grad_norm  9.31 | loss  2.30 | time 42.4783\n",
      "| epoch   7 |   201/  800 batches | grad_norm  9.24 | loss  4.52 | time 42.9358\n",
      "| epoch   7 |   301/  800 batches | grad_norm  7.18 | loss  1.90 | time 43.1394\n",
      "| epoch   7 |   401/  800 batches | grad_norm 19.09 | loss  8.76 | time 42.6839\n",
      "| epoch   7 |   501/  800 batches | grad_norm  4.35 | loss  1.40 | time 41.4966\n",
      "| epoch   7 |   601/  800 batches | grad_norm 11.00 | loss  5.07 | time 40.9674\n",
      "| epoch   7 |   701/  800 batches | grad_norm 10.70 | loss  2.90 | time 41.3419\n",
      "| epoch   7 | train_loss :  5.5127\n",
      "| epoch   7 | valid_loss :  3.4404 accuracy : 0.9295\n",
      "| epoch   8 |     1/  800 batches | grad_norm 22.06 | loss 11.88 | time 0.4414\n",
      "| epoch   8 |   101/  800 batches | grad_norm 25.19 | loss  7.68 | time 41.6753\n",
      "| epoch   8 |   201/  800 batches | grad_norm 12.96 | loss  3.44 | time 41.1911\n",
      "| epoch   8 |   301/  800 batches | grad_norm 13.62 | loss  4.10 | time 41.1889\n",
      "| epoch   8 |   401/  800 batches | grad_norm  3.10 | loss  0.74 | time 41.2891\n",
      "| epoch   8 |   501/  800 batches | grad_norm 12.39 | loss  7.12 | time 41.6743\n",
      "| epoch   8 |   601/  800 batches | grad_norm  7.71 | loss  3.01 | time 40.9824\n",
      "| epoch   8 |   701/  800 batches | grad_norm 17.98 | loss  5.58 | time 42.0921\n",
      "| epoch   8 | train_loss :  4.8319\n",
      "| epoch   8 | valid_loss :  3.6036 accuracy : 0.9198\n",
      "| epoch   9 |     1/  800 batches | grad_norm 10.92 | loss  2.69 | time 0.4271\n",
      "| epoch   9 |   101/  800 batches | grad_norm  0.93 | loss  0.29 | time 40.8431\n",
      "| epoch   9 |   201/  800 batches | grad_norm  7.76 | loss  2.25 | time 40.7979\n",
      "| epoch   9 |   301/  800 batches | grad_norm 16.96 | loss  4.91 | time 41.5610\n",
      "| epoch   9 |   401/  800 batches | grad_norm 12.55 | loss  5.22 | time 41.3084\n",
      "| epoch   9 |   501/  800 batches | grad_norm 12.43 | loss  8.03 | time 41.3693\n",
      "| epoch   9 |   601/  800 batches | grad_norm 11.42 | loss  9.22 | time 41.2570\n",
      "| epoch   9 |   701/  800 batches | grad_norm  8.91 | loss  3.31 | time 41.4764\n",
      "| epoch   9 | train_loss :  4.1605\n",
      "| epoch   9 | valid_loss :  6.7443 accuracy : 0.8661\n",
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-03.\n",
      "| epoch  10 |     1/  800 batches | grad_norm  0.70 | loss  0.15 | time 0.4427\n",
      "| epoch  10 |   101/  800 batches | grad_norm  9.07 | loss  2.36 | time 41.3703\n",
      "| epoch  10 |   201/  800 batches | grad_norm 11.15 | loss  5.80 | time 42.3000\n",
      "| epoch  10 |   301/  800 batches | grad_norm 14.23 | loss  4.24 | time 41.6392\n",
      "| epoch  10 |   401/  800 batches | grad_norm 12.88 | loss  3.06 | time 41.2953\n",
      "| epoch  10 |   501/  800 batches | grad_norm 10.60 | loss  3.39 | time 41.6979\n",
      "| epoch  10 |   601/  800 batches | grad_norm  5.82 | loss  1.00 | time 41.0717\n",
      "| epoch  10 |   701/  800 batches | grad_norm  3.27 | loss  0.65 | time 41.3481\n",
      "| epoch  10 | train_loss :  2.5299\n",
      "| epoch  10 | valid_loss :  1.8228 accuracy : 0.9598\n",
      "| epoch  11 |     1/  800 batches | grad_norm  9.63 | loss  2.92 | time 0.4723\n",
      "| epoch  11 |   101/  800 batches | grad_norm 12.78 | loss  4.04 | time 41.9123\n",
      "| epoch  11 |   201/  800 batches | grad_norm  3.87 | loss  0.93 | time 41.4600\n",
      "| epoch  11 |   301/  800 batches | grad_norm 16.68 | loss  5.44 | time 41.5048\n",
      "| epoch  11 |   401/  800 batches | grad_norm  8.94 | loss  1.82 | time 41.2262\n",
      "| epoch  11 |   501/  800 batches | grad_norm 19.56 | loss  7.59 | time 41.4552\n",
      "| epoch  11 |   601/  800 batches | grad_norm 12.86 | loss  2.01 | time 40.9018\n",
      "| epoch  11 |   701/  800 batches | grad_norm 20.82 | loss  8.09 | time 41.2711\n",
      "| epoch  11 | train_loss :  2.1910\n",
      "| epoch  11 | valid_loss :  1.8820 accuracy : 0.9596\n",
      "| epoch  12 |     1/  800 batches | grad_norm  9.81 | loss  0.97 | time 0.4279\n",
      "| epoch  12 |   101/  800 batches | grad_norm  0.42 | loss  0.05 | time 41.1910\n",
      "| epoch  12 |   201/  800 batches | grad_norm  2.73 | loss  0.42 | time 40.9255\n",
      "| epoch  12 |   301/  800 batches | grad_norm  7.66 | loss  1.45 | time 40.7318\n",
      "| epoch  12 |   401/  800 batches | grad_norm  7.24 | loss  1.48 | time 41.1416\n",
      "| epoch  12 |   501/  800 batches | grad_norm 12.61 | loss  2.64 | time 41.1674\n",
      "| epoch  12 |   601/  800 batches | grad_norm  3.94 | loss  0.49 | time 40.7223\n",
      "| epoch  12 |   701/  800 batches | grad_norm 10.57 | loss  2.76 | time 40.7255\n",
      "| epoch  12 | train_loss :  2.0294\n",
      "| epoch  12 | valid_loss :  1.8321 accuracy : 0.9564\n",
      "Epoch    12: reducing learning rate of group 0 to 2.5000e-03.\n",
      "| epoch  13 |     1/  800 batches | grad_norm  3.78 | loss  0.74 | time 0.4526\n",
      "| epoch  13 |   101/  800 batches | grad_norm 10.85 | loss  2.38 | time 41.9201\n",
      "| epoch  13 |   201/  800 batches | grad_norm  8.85 | loss  1.67 | time 42.5055\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 50 # 100\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    # train_one_epoch\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion,  optimizer=optimizer, device=device, epoch=epoch)\n",
    "    print(\"| epoch {:3d} | train_loss : {:7.4f}\".format(epoch+1, train_loss))\n",
    "    \n",
    "    # validate_one_epoch\n",
    "    valid_loss, accuracy = validate_one_epoch(model, valid_loader, criterion, optimizer=None, device=device, epoch=epoch, print_output=True)\n",
    "    print(\"| epoch {:3d} | valid_loss : {:7.4f} accuracy : {:.4f}\".format(epoch+1, valid_loss, accuracy))\n",
    "    \n",
    "    # save_model\n",
    "    # save_model()\n",
    "    \n",
    "    lr_scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_p = []\n",
    "result_d = []\n",
    "\n",
    "for batch, (data, label) in enumerate(test_loader):\n",
    "    # data\n",
    "    data = data.to(device)\n",
    "    label = label.to(device)\n",
    "    \n",
    "    hyp = model(data)\n",
    "    \n",
    "    pred = torch.argmax(hyp, dim=-1)\n",
    "    \n",
    "    sample_num = pred.size(0)\n",
    "    for i in range(sample_num):\n",
    "        pred_idx = pred[i].detach().item()\n",
    "        \n",
    "        pred_plant, pred_disease = index2label[pred_idx]\n",
    "        result_p.append(pred_plant)\n",
    "        result_d.append(pred_disease)\n",
    "        \n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"./test.tsv\", sep='\\t', header=None)\n",
    "# print(data_test)\n",
    "\n",
    "data_test[1] = result_p\n",
    "data_test[2] = result_d\n",
    "print(data_test)\n",
    "\n",
    "data_test.to_csv(\"test_result.tsv\", index=False, header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
